#  Jarvis  
### Your Personal Offline AI Assistant (FastAPI + Ollama + Docker)

![Jarvis Banner](https://img.shields.io/badge/Project-Offline%20Jarvis-blue?style=for-the-badge)
![FastAPI](https://img.shields.io/badge/Backend-FastAPI-green?style=for-the-badge&logo=fastapi)
![Docker](https://img.shields.io/badge/Deployed%20with-Docker-blue?style=for-the-badge&logo=docker)
![Python](https://img.shields.io/badge/Language-Python-yellow?style=for-the-badge&logo=python)
![License](https://img.shields.io/badge/License-MIT-lightgrey?style=for-the-badge)

---

## ğŸ§© Overview
**Offline Jarvis** is a lightweight AI chatbot that runs completely **offline** using  
**[FastAPI](https://fastapi.tiangolo.com/)** for the backend and **[Ollama](https://ollama.com/)** for local LLM inference.  
It uses **Llama 3.2:3B** (or any Ollama-supported model) and features a clean, minimal **web interface**.

Perfect for developers who want an offline ChatGPT-like assistant!

---

## âœ¨ Features
- ğŸ’¬ Modern and responsive **chat interface**
- âš™ï¸ **FastAPI** backend with `/chat` endpoint
- ğŸ§± **Runs entirely offline** â€” no API keys or cloud calls
- ğŸ§  Powered by **Ollama** (supports Gemma, Llama, Phi, etc.)
- ğŸ“‹ Copy-to-clipboard for AI responses
- ğŸª¶ Markdown + Syntax highlighting for code blocks
- ğŸ³ One-command **Docker deployment**

---


## âš™ï¸ Requirements
Before you start, make sure you have:
- ğŸ **Python 3.10+** (optional if not using Docker)
- ğŸ‹ **Docker** installed  
- ğŸ¤– **Ollama** installed and running locally  
- ğŸ§  Model pulled (e.g., `llama3.2:3b`)

### Pull the Model:

```bash
ollama pull llama3.2:3b
```
If you want a different model (like gemma:2b or phi3), replace the model name accordingly.
---

## ğŸš€ Setup and Run

ğŸ§± Step 1 â€” Build the Docker Image

Run this command in the project root (where your Dockerfile and app.py are located):
```bash
docker build -t offline-jarvis .
```
This command:
	â€¢	Builds the Docker image.
	â€¢	Tags it as offline-jarvis.

â¸»

## â–¶ï¸ Step 2 â€” Run the Container

Once the image is built, start your container:
```bash
docker run -d -p 8000:8000 --name offline-jarvis -v ~/.ollama:/root/.ollama offline-jarvis
```
Explanation:
	â€¢	-d â†’ Run container in detached mode (background)
	â€¢	-p 8000:8000 â†’ Map port 8000 on host to port 8000 in container
	â€¢	--name offline-jarvis â†’ Assigns a name to the container
	â€¢	-v ~/.ollama:/root/.ollama â†’ Mounts local Ollama data inside container

â¸»

ğŸŒ Step 3 â€” Open in Browser

Once the container is running, open your browser and visit:

ğŸ‘‰ http://localhost:8000/web

Youâ€™ll see your Offline Jarvis chatbot ready to chat ğŸ’¬
### ğŸ§± Troubleshooting

âŒ Container name already in use
```bash
docker rm -f offline-jarvis
```
## ollama not running
```bash
ollama serve
```
### ğŸ‘¨â€ğŸ’» Author

### Sandeep manchinasetti
ğŸ’¡ Built with â¤ï¸ using FastAPI, Ollama, and Docker

â¸»

### ğŸ“ License

This project is licensed under the MIT License â€” feel free to use, modify, and share.

â¸»

â­ Star this repo if you found it helpful!






